{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Flatten, Convolution2D, MaxPooling2D, Permute, Dropout, Reshape, LSTM, BatchNormalization,TimeDistributed\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import Adam #, RMSprop, SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from time import time\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scenes\n",
    "batch_size =  64\n",
    "nb_classes = 42\n",
    "# input dimensions  \n",
    "rows, cols , channels = 1292, 128,2    \n",
    "#model params\n",
    "nb_epoch = 50\n",
    "pool_size = (3,3)                  # size of pooling area for max pooling\n",
    "prob_drop_conv = 0.25                # drop probability for dropout @ conv layer\n",
    "prob_drop_hidden = 0.5              # drop probability for dropout @ fc layer\n",
    "fold=1\n",
    "%run shared_functions.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D data - two channel input!\n",
    "\n",
    "#Load yogi\n",
    "#import random \n",
    "\n",
    "def getLabelMat(scene):\n",
    "    with open(scene, \"rb\" ) as scenesample:\n",
    "        labelMat = pickle.load(scenesample)\n",
    "    return labelMat\n",
    "\n",
    "def getStackFeatMat(scene):\n",
    "    with open(scene, \"rb\" ) as scenesample:\n",
    "        l,fv = pickle.load(scenesample)\n",
    "    return fv,l\n",
    "\n",
    "def numerise(labmat, nb_classes):\n",
    "    r,c=labmat.shape\n",
    "    nummat=np.zeros((labmat.shape))\n",
    "    for i in range(0,r):\n",
    "        for j in range(0,c):\n",
    "            if labmat[i,j]==1:\n",
    "                nummat[i,j]=i\n",
    "    return np_utils.to_categorical(nummat, nb_classes)\n",
    "\n",
    "def reduceLabels(labels):\n",
    "    r,c=labels.shape  #(should be 1292,42)\n",
    "    npOfLabels=np.zeros(c)\n",
    "    for j in range(0,c):\n",
    "        if np.sum(labels[:,j])>0:\n",
    "            npOfLabels[j]=1\n",
    "    print(npOfLabels.shape)\n",
    "    return npOfLabels\n",
    "\n",
    "def getTrainDataForFold(foldNo):\n",
    "    n_bg,bg_labels=countBackgrounds('inputsounds\\\\background\\\\train')\n",
    "    src_files='stacked_for_classification\\\\train\\\\'+str(foldNo)+'\\\\*'\n",
    "    validationIDs = random.sample(range(1,9), 1)\n",
    "    print(validationIDs)\n",
    "    vf_list=[]\n",
    "    tf_list=[]\n",
    "    vl_list=[]\n",
    "    tl_list=[]\n",
    "    labels={}\n",
    "    \n",
    "    while len(vf_list)==0:\n",
    "        for bgl in bg_labels.keys():\n",
    "            s_files=src_files+bgl+'_*2channel.pckl'\n",
    "            filelist=glob.glob(s_files)\n",
    "            for trainfile in filelist:  #this list is 150 files long, ie 3pitch*50 samples per 1 class\n",
    "                src=trainfile.split('\\\\')[-1].split('_')[0]\n",
    "                bgs=''.join(i for i in src if i.isdigit())\n",
    "                if int(bgs) in validationIDs: #then is a validation file\n",
    "                     vf_list.append(trainfile)\n",
    "                else:\n",
    "                    tf_list.append(trainfile)\n",
    "                \n",
    "    print('training list is '+str(len(tf_list))+' long')\n",
    "    print('validation list is '+str(len(vf_list))+' long')\n",
    "    \n",
    "\n",
    "    #now go through filelist (tf_list) to create X_train; should be 2100,128,1292,2\n",
    "    X_train=np.zeros((len(tf_list),1292,128,2))\n",
    "    Y_train=np.zeros((len(tf_list),1292,nb_classes))\n",
    "    \n",
    "    for i,tfile in enumerate(tf_list):\n",
    "        fv,l=getStackFeatMat(tfile)\n",
    "        X_train[i,:,:,:]=np.swapaxes(fv,0,1)\n",
    "        Y_train[i,:]=l\n",
    "    return X_train, Y_train, tf_list, vf_list\n",
    "\n",
    "#X_train, Y_train, tf_list, vf_list = getTrainDataForFold(fold)\n",
    "\n",
    "#print('ta da')\n",
    "#print(X_train.shape)\n",
    "#print(Y_train.shape)\n",
    "\n",
    "#setup validation data\n",
    "def getValDataForFold(vf_list):\n",
    "#now go through validation (vf_list) to create X_train; should be 960,128,1292,2\n",
    "    X_val=np.zeros((len(vf_list),1292,128,2))\n",
    "    Y_val=np.zeros((len(vf_list),1292,nb_classes))\n",
    "    for i,vfile in enumerate(vf_list):\n",
    "        fv,l=getStackFeatMat(vfile)\n",
    "        X_val[i,:,:,:]=np.swapaxes(fv,0,1)\n",
    "        Y_val[i,:]=l\n",
    "    return X_val, Y_val\n",
    "\n",
    "#X_val,Y_val=getValDataForFold(vf_list)\n",
    "#print('ta da')\n",
    "#print(X_val.shape)\n",
    "#print(Y_val.shape)\n",
    "\n",
    "def getTestDataForFold(tef_list):\n",
    "    print('testing list is '+str(len(tef_list))+' long')\n",
    "    X_test=np.zeros((len(tef_list),1292,128,2))\n",
    "    Y_test=np.zeros((len(tef_list),1292,nb_classes))    \n",
    "    for i,tefile in enumerate(tef_list):\n",
    "        fv,l=getStackFeatMat(tefile)\n",
    "        X_test[i,:,:,:]=np.swapaxes(fv,0,1)\n",
    "        Y_test[i,:]=l\n",
    "    return X_test, Y_test\n",
    "\n",
    "#src_files='stacked_for_classification\\\\test\\\\'+str(fold)+'\\\\*2channel.pckl'\n",
    "#tef_list=glob.glob(src_files)\n",
    "#X_test,Y_test=getTestDataForFold(tef_list)\n",
    "#print(X_test.shape)\n",
    "#print('test data sorted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 1292, 128, 256)    4864      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 646, 64, 256)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 646, 64, 256)      1024      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 646, 64, 256)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 646, 64, 512)      1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 323, 32, 512)      0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 323, 32, 512)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 323, 32, 1292)     2647308   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 162, 16, 1292)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 162, 16, 1292)     5168      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 162, 16, 1292)     0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 1292, 2592)        0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1292, 1292)        20077680  \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 1292, 1292)        1670556   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1292, 1292)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1292, 1292)        5168      \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 1292, 42)          54306     \n",
      "=================================================================\n",
      "Total params: 25,646,234\n",
      "Trainable params: 25,640,554\n",
      "Non-trainable params: 5,680\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#scenes\n",
    "batch_size =  32\n",
    "nb_classes = 42\n",
    "# input dimensions  \n",
    "rows, cols , channels = 1292, 128,2    \n",
    "#model params\n",
    "nb_epoch = 50\n",
    "pool_size = (3,3)                  # size of pooling area for max pooling\n",
    "prob_drop_conv = 0.25                # drop probability for dropout @ conv layer\n",
    "prob_drop_hidden = 0.5              # drop probability for dropout @ fc layer\n",
    "\n",
    "\n",
    "###### Convolutional model\n",
    "def compileCRNN(cols,rows,channels,nb_classes=1):\n",
    "    model = Sequential()\n",
    "    # conv1 layer\n",
    "   #  model.add(Convolution1D(16, (3), padding='same', activation='relu', input_shape=(cols,channels)))\n",
    "   \n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu', input_shape=(rows,cols,channels)))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size, strides=(2), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "    # conv2 layer\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size, strides=(2), padding='same'))\n",
    "    model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "    # conv3 layer\n",
    "    model.add(Conv2D(1292, (2, 2), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size, strides=(2), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "   # model.add(Flatten())\n",
    "    model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "    # lstm layer\n",
    "    model.add(Reshape((1292,-1)))\n",
    "    #model.add(Permute((2,1)))\n",
    "    model.add(LSTM(1292,input_shape=(rows, cols),return_sequences=True))\n",
    "  #  model.add(Reshape((256,-1)))\n",
    "\n",
    "    # fc1 layer\n",
    "    model.add(TimeDistributed(Dense(1292, activation='relu')))\n",
    "    model.add(Dropout(prob_drop_hidden))\n",
    "    \n",
    "    # fc2 layer\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(TimeDistributed(Dense(nb_classes, activation='sigmoid')))\n",
    "    \n",
    "    opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    #opt = RMSprop(lr=0.001, rho=0.9)\n",
    "    #opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = compileCRNN(cols,rows,channels,nb_classes=nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "#fold=1\n",
    "def train_network(model, model_name, X_train, Y_train, X_val, Y_val, nb_epoch, validationsplit_size, batchsize, early_stoping_patience, output_folder):\n",
    "    checkpointer = ModelCheckpoint(filepath=os.path.join(output_folder,model_name + '.hdf5'),save_best_only=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=early_stoping_patience)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs\\\\{}\".format(time()))\n",
    "    Callbacks=[checkpointer,  tensorboard] #early_stopping,\n",
    "  #  print(samples)\n",
    "    steps=int(samples/batchsize)\n",
    "    validationsteps=int(validationsplit_size/batchsize)\n",
    "    history = model.fit(X_train, Y_train, epochs=nb_epoch, callbacks=Callbacks, batch_size=batch_size, validation_data=(X_val, Y_val), shuffle=True, verbose=1)\n",
    "    return history,model\n",
    "\n",
    "def buildModel(savemodelfilename, samples,model,X_train, Y_train,X_val, Y_val,fold ):\n",
    "    valSplit_size = int(samples/4)\n",
    "    early_stoping_patience=10\n",
    "    history,model = train_network(model, '2D_models\\\\f'+str(fold)+'-best_model-strongFramesV2', X_train, Y_train, X_val, Y_val, nb_epoch, valSplit_size, batch_size, early_stoping_patience,'.')\n",
    "    model.save_weights(savemodelfilename)\n",
    "    return model, history\n",
    "\n",
    "#testruncode - all fold run in cell below\n",
    "#savemodelfilename='2D_models\\\\f'+str(fold)+'_model.testsave'\n",
    "#samples=(X_train.shape[0])\n",
    "#print(X_train.shape)\n",
    "#models[fold],histories[fold] = buildModel(savemodelfilename, samples,model,X_train_mfcc,Y_train_mfcc,X_val_mfcc, Y_val_mfcc,)\n",
    "#models,histories= buildModel(savemodelfilename, samples,model,X_train,Y_train,X_val, Y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[7]\n",
      "training list is 2100 long\n",
      "validation list is 300 long\n",
      "data obtained\n",
      "lists sorted\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 1292, 128, 256)    4864      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 646, 64, 256)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 646, 64, 256)      1024      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 646, 64, 256)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 646, 64, 512)      1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 323, 32, 512)      0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 323, 32, 512)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 323, 32, 1292)     2647308   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 162, 16, 1292)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 162, 16, 1292)     5168      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 162, 16, 1292)     0         \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 1292, 2592)        0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 1292, 1292)        20077680  \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 1292, 1292)        1670556   \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1292, 1292)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1292, 1292)        5168      \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 1292, 42)          54306     \n",
      "=================================================================\n",
      "Total params: 25,646,234\n",
      "Trainable params: 25,640,554\n",
      "Non-trainable params: 5,680\n",
      "_________________________________________________________________\n",
      "model compiled\n",
      "2019-03-21 10:54:16.600912\n",
      "Train on 2100 samples, validate on 300 samples\n",
      "Epoch 1/100\n",
      "2100/2100 [==============================] - 1171s 558ms/step - loss: 0.2497 - binary_accuracy: 0.9058 - val_loss: 0.1510 - val_binary_accuracy: 0.9583\n",
      "Epoch 2/100\n",
      "2100/2100 [==============================] - 1149s 547ms/step - loss: 0.1344 - binary_accuracy: 0.9596 - val_loss: 0.1497 - val_binary_accuracy: 0.9589\n",
      "Epoch 3/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.1114 - binary_accuracy: 0.9626 - val_loss: 0.1264 - val_binary_accuracy: 0.9619\n",
      "Epoch 4/100\n",
      "2100/2100 [==============================] - 1148s 546ms/step - loss: 0.0855 - binary_accuracy: 0.9708 - val_loss: 0.1347 - val_binary_accuracy: 0.9585\n",
      "Epoch 5/100\n",
      "2100/2100 [==============================] - 1149s 547ms/step - loss: 0.0653 - binary_accuracy: 0.9780 - val_loss: 0.1341 - val_binary_accuracy: 0.9596\n",
      "Epoch 6/100\n",
      "2100/2100 [==============================] - 1161s 553ms/step - loss: 0.0577 - binary_accuracy: 0.9802 - val_loss: 0.1461 - val_binary_accuracy: 0.9547\n",
      "Epoch 7/100\n",
      "2100/2100 [==============================] - 1159s 552ms/step - loss: 0.0507 - binary_accuracy: 0.9821 - val_loss: 0.1627 - val_binary_accuracy: 0.9570\n",
      "Epoch 8/100\n",
      "2100/2100 [==============================] - 1157s 551ms/step - loss: 0.0492 - binary_accuracy: 0.9822 - val_loss: 0.1890 - val_binary_accuracy: 0.9548\n",
      "Epoch 9/100\n",
      "2100/2100 [==============================] - 1158s 551ms/step - loss: 0.0506 - binary_accuracy: 0.9817 - val_loss: 0.1689 - val_binary_accuracy: 0.9563\n",
      "Epoch 10/100\n",
      "2100/2100 [==============================] - 1165s 555ms/step - loss: 0.0451 - binary_accuracy: 0.9831 - val_loss: 0.1778 - val_binary_accuracy: 0.9531\n",
      "Epoch 11/100\n",
      "2100/2100 [==============================] - 1159s 552ms/step - loss: 0.0426 - binary_accuracy: 0.9837 - val_loss: 0.1756 - val_binary_accuracy: 0.9567\n",
      "Epoch 12/100\n",
      "2100/2100 [==============================] - 1159s 552ms/step - loss: 0.0412 - binary_accuracy: 0.9840 - val_loss: 0.1639 - val_binary_accuracy: 0.9589\n",
      "Epoch 13/100\n",
      "2100/2100 [==============================] - 1154s 550ms/step - loss: 0.0399 - binary_accuracy: 0.9843 - val_loss: 0.1768 - val_binary_accuracy: 0.9575\n",
      "Epoch 14/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0390 - binary_accuracy: 0.9845 - val_loss: 0.1780 - val_binary_accuracy: 0.9577\n",
      "Epoch 15/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0379 - binary_accuracy: 0.9850 - val_loss: 0.1680 - val_binary_accuracy: 0.9580\n",
      "Epoch 16/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0361 - binary_accuracy: 0.9856 - val_loss: 0.2129 - val_binary_accuracy: 0.9539\n",
      "Epoch 17/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0351 - binary_accuracy: 0.9860 - val_loss: 0.1819 - val_binary_accuracy: 0.9581\n",
      "Epoch 18/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0339 - binary_accuracy: 0.9865 - val_loss: 0.1988 - val_binary_accuracy: 0.9565\n",
      "Epoch 19/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0323 - binary_accuracy: 0.9872 - val_loss: 0.1922 - val_binary_accuracy: 0.9591\n",
      "Epoch 20/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0308 - binary_accuracy: 0.9878 - val_loss: 0.1938 - val_binary_accuracy: 0.9581\n",
      "Epoch 21/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0527 - binary_accuracy: 0.9816 - val_loss: 0.4884 - val_binary_accuracy: 0.9201\n",
      "Epoch 22/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0405 - binary_accuracy: 0.9847 - val_loss: 0.1987 - val_binary_accuracy: 0.9545\n",
      "Epoch 23/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0340 - binary_accuracy: 0.9868 - val_loss: 0.1945 - val_binary_accuracy: 0.9541\n",
      "Epoch 24/100\n",
      "2100/2100 [==============================] - 1148s 546ms/step - loss: 0.0313 - binary_accuracy: 0.9878 - val_loss: 0.2109 - val_binary_accuracy: 0.9536\n",
      "Epoch 25/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0294 - binary_accuracy: 0.9885 - val_loss: 0.1976 - val_binary_accuracy: 0.9568\n",
      "Epoch 26/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0283 - binary_accuracy: 0.9889 - val_loss: 0.2097 - val_binary_accuracy: 0.9557\n",
      "Epoch 27/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0267 - binary_accuracy: 0.9895 - val_loss: 0.2063 - val_binary_accuracy: 0.9554\n",
      "Epoch 28/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0255 - binary_accuracy: 0.9901 - val_loss: 0.2186 - val_binary_accuracy: 0.9578\n",
      "Epoch 29/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0240 - binary_accuracy: 0.9906 - val_loss: 0.2426 - val_binary_accuracy: 0.9554\n",
      "Epoch 30/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0234 - binary_accuracy: 0.9909 - val_loss: 0.2208 - val_binary_accuracy: 0.9576\n",
      "Epoch 31/100\n",
      "2100/2100 [==============================] - 1148s 546ms/step - loss: 0.0225 - binary_accuracy: 0.9912 - val_loss: 0.2496 - val_binary_accuracy: 0.9552\n",
      "Epoch 32/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0209 - binary_accuracy: 0.9918 - val_loss: 0.2431 - val_binary_accuracy: 0.9549\n",
      "Epoch 33/100\n",
      "2100/2100 [==============================] - 1148s 547ms/step - loss: 0.0199 - binary_accuracy: 0.9923 - val_loss: 0.2644 - val_binary_accuracy: 0.9569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0189 - binary_accuracy: 0.9927 - val_loss: 0.2919 - val_binary_accuracy: 0.9522\n",
      "Epoch 35/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0182 - binary_accuracy: 0.9930 - val_loss: 0.2771 - val_binary_accuracy: 0.9567\n",
      "Epoch 36/100\n",
      "2100/2100 [==============================] - 1148s 546ms/step - loss: 0.0385 - binary_accuracy: 0.9866 - val_loss: 0.2488 - val_binary_accuracy: 0.9516\n",
      "Epoch 37/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0238 - binary_accuracy: 0.9910 - val_loss: 0.2469 - val_binary_accuracy: 0.9516\n",
      "Epoch 38/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0197 - binary_accuracy: 0.9925 - val_loss: 0.2589 - val_binary_accuracy: 0.9525\n",
      "Epoch 39/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0180 - binary_accuracy: 0.9932 - val_loss: 0.2901 - val_binary_accuracy: 0.9490\n",
      "Epoch 40/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0162 - binary_accuracy: 0.9938 - val_loss: 0.2812 - val_binary_accuracy: 0.9486\n",
      "Epoch 41/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0155 - binary_accuracy: 0.9941 - val_loss: 0.3191 - val_binary_accuracy: 0.9488\n",
      "Epoch 42/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0148 - binary_accuracy: 0.9944 - val_loss: 0.3255 - val_binary_accuracy: 0.9499\n",
      "Epoch 43/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0140 - binary_accuracy: 0.9948 - val_loss: 0.3133 - val_binary_accuracy: 0.9465\n",
      "Epoch 44/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0131 - binary_accuracy: 0.9951 - val_loss: 0.3349 - val_binary_accuracy: 0.9486\n",
      "Epoch 45/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0127 - binary_accuracy: 0.9953 - val_loss: 0.3360 - val_binary_accuracy: 0.9476\n",
      "Epoch 46/100\n",
      "2100/2100 [==============================] - 1150s 547ms/step - loss: 0.0123 - binary_accuracy: 0.9954 - val_loss: 0.3197 - val_binary_accuracy: 0.9511\n",
      "Epoch 47/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0117 - binary_accuracy: 0.9957 - val_loss: 0.3417 - val_binary_accuracy: 0.9495\n",
      "Epoch 48/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0113 - binary_accuracy: 0.9958 - val_loss: 0.3423 - val_binary_accuracy: 0.9480\n",
      "Epoch 49/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0113 - binary_accuracy: 0.9958 - val_loss: 0.3880 - val_binary_accuracy: 0.9479\n",
      "Epoch 50/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0105 - binary_accuracy: 0.9962 - val_loss: 0.3892 - val_binary_accuracy: 0.9475\n",
      "Epoch 51/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0099 - binary_accuracy: 0.9964 - val_loss: 0.3582 - val_binary_accuracy: 0.9498\n",
      "Epoch 52/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0095 - binary_accuracy: 0.9965 - val_loss: 0.3876 - val_binary_accuracy: 0.9468\n",
      "Epoch 53/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0094 - binary_accuracy: 0.9966 - val_loss: 0.3975 - val_binary_accuracy: 0.9507\n",
      "Epoch 54/100\n",
      "2100/2100 [==============================] - 1148s 547ms/step - loss: 0.0091 - binary_accuracy: 0.9967 - val_loss: 0.3880 - val_binary_accuracy: 0.9459\n",
      "Epoch 55/100\n",
      "2100/2100 [==============================] - 1150s 548ms/step - loss: 0.0090 - binary_accuracy: 0.9967 - val_loss: 0.3989 - val_binary_accuracy: 0.9472\n",
      "Epoch 56/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0084 - binary_accuracy: 0.9970 - val_loss: 0.4004 - val_binary_accuracy: 0.9487\n",
      "Epoch 57/100\n",
      "2100/2100 [==============================] - 1149s 547ms/step - loss: 0.0076 - binary_accuracy: 0.9973 - val_loss: 0.3894 - val_binary_accuracy: 0.9484\n",
      "Epoch 58/100\n",
      "2100/2100 [==============================] - 1149s 547ms/step - loss: 0.0076 - binary_accuracy: 0.9973 - val_loss: 0.4710 - val_binary_accuracy: 0.9456\n",
      "Epoch 59/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0081 - binary_accuracy: 0.9971 - val_loss: 0.4044 - val_binary_accuracy: 0.9501\n",
      "Epoch 60/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0074 - binary_accuracy: 0.9973 - val_loss: 0.3823 - val_binary_accuracy: 0.9497\n",
      "Epoch 61/100\n",
      "2100/2100 [==============================] - 1148s 547ms/step - loss: 0.0072 - binary_accuracy: 0.9974 - val_loss: 0.4101 - val_binary_accuracy: 0.9495\n",
      "Epoch 62/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0066 - binary_accuracy: 0.9977 - val_loss: 0.4009 - val_binary_accuracy: 0.9501\n",
      "Epoch 63/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0064 - binary_accuracy: 0.9977 - val_loss: 0.4550 - val_binary_accuracy: 0.9514\n",
      "Epoch 64/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0230 - binary_accuracy: 0.9932 - val_loss: 0.3083 - val_binary_accuracy: 0.9371\n",
      "Epoch 65/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0203 - binary_accuracy: 0.9931 - val_loss: 0.2180 - val_binary_accuracy: 0.9576\n",
      "Epoch 66/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0103 - binary_accuracy: 0.9964 - val_loss: 0.2741 - val_binary_accuracy: 0.9554\n",
      "Epoch 67/100\n",
      "2100/2100 [==============================] - 1148s 547ms/step - loss: 0.0082 - binary_accuracy: 0.9971 - val_loss: 0.2992 - val_binary_accuracy: 0.9521\n",
      "Epoch 68/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0070 - binary_accuracy: 0.9975 - val_loss: 0.2963 - val_binary_accuracy: 0.9533\n",
      "Epoch 69/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0064 - binary_accuracy: 0.9978 - val_loss: 0.3153 - val_binary_accuracy: 0.9551\n",
      "Epoch 70/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0063 - binary_accuracy: 0.9978 - val_loss: 0.3367 - val_binary_accuracy: 0.9546\n",
      "Epoch 71/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0058 - binary_accuracy: 0.9980 - val_loss: 0.3182 - val_binary_accuracy: 0.9529\n",
      "Epoch 72/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0057 - binary_accuracy: 0.9980 - val_loss: 0.3593 - val_binary_accuracy: 0.9549\n",
      "Epoch 73/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0052 - binary_accuracy: 0.9982 - val_loss: 0.3657 - val_binary_accuracy: 0.9522\n",
      "Epoch 74/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0052 - binary_accuracy: 0.9982 - val_loss: 0.3283 - val_binary_accuracy: 0.9563\n",
      "Epoch 75/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0051 - binary_accuracy: 0.9983 - val_loss: 0.3244 - val_binary_accuracy: 0.9557\n",
      "Epoch 76/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0059 - binary_accuracy: 0.9979 - val_loss: 0.4083 - val_binary_accuracy: 0.9501\n",
      "Epoch 77/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0055 - binary_accuracy: 0.9981 - val_loss: 0.3762 - val_binary_accuracy: 0.9510\n",
      "Epoch 78/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0051 - binary_accuracy: 0.9982 - val_loss: 0.3163 - val_binary_accuracy: 0.9544\n",
      "Epoch 79/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0048 - binary_accuracy: 0.9983 - val_loss: 0.4042 - val_binary_accuracy: 0.9506\n",
      "Epoch 80/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0045 - binary_accuracy: 0.9984 - val_loss: 0.4261 - val_binary_accuracy: 0.9490\n",
      "Epoch 81/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0044 - binary_accuracy: 0.9985 - val_loss: 0.3867 - val_binary_accuracy: 0.9509\n",
      "Epoch 82/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0044 - binary_accuracy: 0.9985 - val_loss: 0.4204 - val_binary_accuracy: 0.9506\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0046 - binary_accuracy: 0.9984 - val_loss: 0.4481 - val_binary_accuracy: 0.9496\n",
      "Epoch 84/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0045 - binary_accuracy: 0.9985 - val_loss: 0.4430 - val_binary_accuracy: 0.9506\n",
      "Epoch 85/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0046 - binary_accuracy: 0.9984 - val_loss: 0.4211 - val_binary_accuracy: 0.9515\n",
      "Epoch 86/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0043 - binary_accuracy: 0.9986 - val_loss: 0.3854 - val_binary_accuracy: 0.9530\n",
      "Epoch 87/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0047 - binary_accuracy: 0.9984 - val_loss: 0.4417 - val_binary_accuracy: 0.9462\n",
      "Epoch 88/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0048 - binary_accuracy: 0.9984 - val_loss: 0.4386 - val_binary_accuracy: 0.9515\n",
      "Epoch 89/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0043 - binary_accuracy: 0.9985 - val_loss: 0.4258 - val_binary_accuracy: 0.9546\n",
      "Epoch 90/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0046 - binary_accuracy: 0.9984 - val_loss: 0.4696 - val_binary_accuracy: 0.9475\n",
      "Epoch 91/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0038 - binary_accuracy: 0.9987 - val_loss: 0.4740 - val_binary_accuracy: 0.9507\n",
      "Epoch 92/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0042 - binary_accuracy: 0.9986 - val_loss: 0.3841 - val_binary_accuracy: 0.9511\n",
      "Epoch 93/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0283 - binary_accuracy: 0.9921 - val_loss: 0.3299 - val_binary_accuracy: 0.9503\n",
      "Epoch 94/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0090 - binary_accuracy: 0.9969 - val_loss: 0.3311 - val_binary_accuracy: 0.9521\n",
      "Epoch 95/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0068 - binary_accuracy: 0.9977 - val_loss: 0.4137 - val_binary_accuracy: 0.9465\n",
      "Epoch 96/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0052 - binary_accuracy: 0.9983 - val_loss: 0.3860 - val_binary_accuracy: 0.9471\n",
      "Epoch 97/100\n",
      "2100/2100 [==============================] - 1146s 546ms/step - loss: 0.0044 - binary_accuracy: 0.9985 - val_loss: 0.4325 - val_binary_accuracy: 0.9463\n",
      "Epoch 98/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0040 - binary_accuracy: 0.9987 - val_loss: 0.4419 - val_binary_accuracy: 0.9446\n",
      "Epoch 99/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0038 - binary_accuracy: 0.9987 - val_loss: 0.4580 - val_binary_accuracy: 0.9445\n",
      "Epoch 100/100\n",
      "2100/2100 [==============================] - 1147s 546ms/step - loss: 0.0035 - binary_accuracy: 0.9988 - val_loss: 0.4511 - val_binary_accuracy: 0.9461\n",
      "2019-03-22 18:48:06.541122\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "batch_size =  8\n",
    "nb_classes = 42\n",
    "rows, cols , channels = 1292, 128,2    \n",
    "nb_epoch = 100\n",
    "pool_size = (3,3)                  # size of pooling area for max pooling\n",
    "prob_drop_conv = 0.25                # drop probability for dropout @ conv layer\n",
    "prob_drop_hidden = 0.5              # drop probability for dropout @ fc layer\n",
    "#fold=1\n",
    "\n",
    "for fold in range(1,2):\n",
    "    print(fold)\n",
    "    #sort data\n",
    "    X_train, Y_train, tf_list, vf_list = getTrainDataForFold(fold)\n",
    "    print('data obtained')\n",
    "    X_val,Y_val=getValDataForFold(vf_list)\n",
    "    print('lists sorted')\n",
    "    model = compileCRNN(cols,rows,channels,nb_classes=nb_classes)\n",
    "    print('model compiled')\n",
    "    savemodelfilename='2D_models\\\\f'+str(fold)+'_model-strongFramesV2.testsave'\n",
    "    samples=(X_train.shape[0])\n",
    "    print(datetime.now())\n",
    "    models,histories= buildModel(savemodelfilename, samples,model,X_train,Y_train,X_val, Y_val,fold)\n",
    "    print(datetime.now())\n",
    "    print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getValDataForFold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-da5f2b1f2cb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_val\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetValDataForFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvf_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'getValDataForFold' is not defined"
     ]
    }
   ],
   "source": [
    "X_val,Y_val=getValDataForFold(vf_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proper ASC - majority voting prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proper SED - on/off set detection? \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
