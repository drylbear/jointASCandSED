{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Flatten, Convolution2D, MaxPooling2D, Permute, Dropout, Reshape, LSTM, BatchNormalization,TimeDistributed\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import Adam #, RMSprop, SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from time import time\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scenes\n",
    "batch_size =  16\n",
    "nb_classes = 10\n",
    "# input dimensions  \n",
    "rows, cols,channels = 1292, 128,1\n",
    "#model params\n",
    "nb_epoch = 100\n",
    "pool_size = (3,3)                  # size of pooling area for max pooling\n",
    "prob_drop_conv = 0.25                # drop probability for dropout @ conv layer\n",
    "prob_drop_hidden = 0.5              # drop probability for dropout @ fc layer\n",
    "fold=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load yogi\n",
    "#import random \n",
    "%run shared_functions.ipynb\n",
    "\n",
    "def getLabelMat(scene):\n",
    "    with open(scene, \"rb\" ) as scenesample:\n",
    "        labelMat = pickle.load(scenesample)\n",
    "    return labelMat\n",
    "\n",
    "def getStackFeatMat(scene):\n",
    "    with open(scene, \"rb\" ) as scenesample:\n",
    "        l,fv = pickle.load(scenesample)\n",
    "    return fv,l\n",
    "\n",
    "def numerise(labmat, nb_classes):\n",
    "    r,c=labmat.shape\n",
    "    nummat=np.zeros((labmat.shape))\n",
    "    for i in range(0,r):\n",
    "        for j in range(0,c):\n",
    "            if labmat[i,j]==1:\n",
    "                nummat[i,j]=i\n",
    "    return np_utils.to_categorical(nummat, nb_classes)\n",
    "\n",
    "def reduceLabels(labels):\n",
    "    r,c=labels.shape  #(should be 1292,42)\n",
    "    npOfLabels=np.zeros(c)\n",
    "    for j in range(0,c):\n",
    "        if np.sum(labels[:,j])>0:\n",
    "            npOfLabels[j]=1\n",
    " #   print(npOfLabels.shape)\n",
    "    return npOfLabels\n",
    "\n",
    "\n",
    "def getTrainDataForFold(foldNo,context):\n",
    "    n_bg,bg_labels=countBackgrounds('inputsounds\\\\background\\\\train')\n",
    "    src_files='stacked_for_classification\\\\train\\\\'+str(foldNo)+'\\\\*'\n",
    "    \n",
    "    vl_list=[]\n",
    "    tl_list=[]\n",
    "    labels={}\n",
    "    vf_list=[]\n",
    "    while len(vf_list)==0:\n",
    "        vf_list=[]\n",
    "        tf_list=[]\n",
    "        \n",
    "        validationIDs = random.sample(range(1,9), 1)\n",
    "        print(validationIDs)\n",
    "        for bgl in bg_labels.keys():\n",
    "            s_files=src_files+bgl+'_*2channel.pckl'\n",
    "            filelist=glob.glob(s_files)\n",
    "            for trainfile in filelist:  #this list is 150 files long, ie 3pitch*50 samples per 1 class\n",
    "                src=trainfile.split('\\\\')[-1].split('_')[0]\n",
    "                bgs=''.join(i for i in src if i.isdigit())\n",
    "                if int(bgs) in validationIDs: #then is a validation file\n",
    "                     vf_list.append(trainfile)\n",
    "                else:\n",
    "                    tf_list.append(trainfile)\n",
    "                \n",
    "    print('training list is '+str(len(tf_list))+' long')\n",
    "    print('validation list is '+str(len(vf_list))+' long')\n",
    "    \n",
    "\n",
    "    #now go through filelist (tf_list) to create X_train; should be 2100,128,1292\n",
    "    X_train=np.zeros((len(tf_list),1292,128))\n",
    "    Y_train=np.zeros((len(tf_list),1292,nb_classes))\n",
    "    \n",
    "    for i,tfile in enumerate(tf_list):\n",
    "        fv,l=getStackFeatMat(tfile)\n",
    "        if context=='asc':\n",
    "            fv=fv[:,:,0]\n",
    "            l=l[:,0:10]\n",
    "        else:\n",
    "            fv=fv[:,:,1]\n",
    "            l=l[:,10:]\n",
    "        X_train[i,:,:]=np.swapaxes(fv,0,1)\n",
    "        Y_train[i,:]=l\n",
    "    return X_train, Y_train, tf_list, vf_list\n",
    "\n",
    "#X_train, Y_train, tf_list, vf_list = getTrainDataForFold(fold,'asc')\n",
    "\n",
    "#print('ta da')\n",
    "#print(X_train.shape)\n",
    "#print(Y_train.shape)\n",
    "\n",
    "#setup validation data\n",
    "def getValDataForFold(vf_list,context):\n",
    "#now go through validation (vf_list) to create X_train; should be 960,128,1292,2\n",
    "    X_val=np.zeros((len(vf_list),1292,128))\n",
    "    Y_val=np.zeros((len(vf_list),1292,nb_classes))\n",
    "    for i,vfile in enumerate(vf_list):\n",
    "        fv,l=getStackFeatMat(vfile)\n",
    "        if context=='asc':\n",
    "            fv=fv[:,:,0]\n",
    "            l=l[:,0:10]\n",
    "        else:\n",
    "            fv=fv[:,:,1]\n",
    "            l=l[:,10:]\n",
    "        X_val[i,:,:]=np.swapaxes(fv,0,1)\n",
    "        Y_val[i,:]=l\n",
    "    return X_val, Y_val\n",
    "\n",
    "#X_val,Y_val=getValDataForFold(vf_list,'asc')\n",
    "#print('ta da')\n",
    "#print(X_val.shape)\n",
    "#print(Y_val.shape)\n",
    "\n",
    "def getTestDataForFold(tef_list,context):\n",
    "    print('testing list is '+str(len(tef_list))+' long')\n",
    "    X_test=np.zeros((len(tef_list),1292,128))\n",
    "    Y_test=np.zeros((len(tef_list),1292,nb_classes))    \n",
    "    for i,tefile in enumerate(tef_list):\n",
    "        fv,l=getStackFeatMat(tefile)\n",
    "        if context=='asc':\n",
    "            fv=fv[:,:,0]\n",
    "            l=l[0:10,:]\n",
    "        else:\n",
    "            fv=fv[:,:,1]\n",
    "            l=l[:,10:]\n",
    "        X_test[i,:,:]=np.swapaxes(fv,0,1)\n",
    "        Y_test[i,:]=l\n",
    "    return X_test, Y_test\n",
    "\n",
    "src_files='stacked_for_classification\\\\test\\\\'+str(fold)+'\\\\*2channel.pckl'\n",
    "tef_list=glob.glob(src_files)\n",
    "#X_test,Y_test=getTestDataForFold(tef_list,'asc')\n",
    "#print(X_test.shape)\n",
    "#print('test data sorted')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 1292, 128, 256)    2560      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 646, 64, 256)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 646, 64, 256)      1024      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 646, 64, 256)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 646, 64, 512)      1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 323, 32, 512)      0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 323, 32, 512)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 323, 32, 1292)     2647308   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 162, 16, 1292)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 162, 16, 1292)     5168      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 162, 16, 1292)     0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 1292, 2592)        0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1292, 1292)        20077680  \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 1292, 1292)        1670556   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1292, 1292)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1292, 1292)        5168      \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 1292, 10)          12930     \n",
      "=================================================================\n",
      "Total params: 25,602,554\n",
      "Trainable params: 25,596,874\n",
      "Non-trainable params: 5,680\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#scenes\n",
    "batch_size =  16\n",
    "nb_classes = 10\n",
    "# input dimensions  \n",
    "rows, cols , channels = 1292, 128,1    \n",
    "#model params\n",
    "nb_epoch = 100\n",
    "pool_size = (3,3)                  # size of pooling area for max pooling\n",
    "prob_drop_conv = 0.25                # drop probability for dropout @ conv layer\n",
    "prob_drop_hidden = 0.5              # drop probability for dropout @ fc layer\n",
    "\n",
    "\n",
    "###### Convolutional model\n",
    "def compileCRNN(cols,rows,channels,nb_classes=1):\n",
    "    model = Sequential()\n",
    "    # conv1 layer \n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu', input_shape=(rows,cols,channels)))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size, strides=(2), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "    # conv2 layer\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size, strides=(2), padding='same'))\n",
    "    model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "    # conv3 layer\n",
    "    model.add(Conv2D(1292, (2, 2), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size, strides=(2), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "   # model.add(Flatten())\n",
    "    model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "    # lstm layer\n",
    "    model.add(Reshape((1292,-1)))\n",
    "    #model.add(Permute((2,1)))\n",
    "    model.add(LSTM(1292,input_shape=(rows, cols),return_sequences=True))\n",
    "  #  model.add(Reshape((256,-1)))\n",
    "\n",
    "    # fc1 layer\n",
    "    model.add(TimeDistributed(Dense(1292, activation='relu')))\n",
    "    model.add(Dropout(prob_drop_hidden))\n",
    "    \n",
    "    # fc2 layer\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(TimeDistributed(Dense(nb_classes, activation='sigmoid')))\n",
    "    \n",
    "    opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    #opt = RMSprop(lr=0.001, rho=0.9)\n",
    "    #opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = compileCRNN(cols,rows,channels,nb_classes=nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "#fold=1\n",
    "def train_network(model, model_name, X_train, Y_train, X_val, Y_val, nb_epoch, validationsplit_size, batchsize, early_stoping_patience, output_folder):\n",
    "    checkpointer = ModelCheckpoint(filepath=os.path.join(output_folder,model_name + '.hdf5'),save_best_only=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=early_stoping_patience)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs\\\\{}\".format(time()))\n",
    "    Callbacks=[checkpointer,  tensorboard] #,early_stopping\n",
    "  #  print(samples)\n",
    "    steps=int(samples/batchsize)\n",
    "    validationsteps=int(validationsplit_size/batchsize)\n",
    "    history = model.fit(X_train, Y_train, epochs=nb_epoch, callbacks=Callbacks, batch_size=batch_size, validation_data=(X_val, Y_val), shuffle=True, verbose=1)\n",
    "    return history,model\n",
    "\n",
    "def buildModel(savemodelfilename, samples,model,X_train, Y_train,X_val, Y_val,fold ):\n",
    "    valSplit_size = int(samples/4)\n",
    "    early_stoping_patience=10\n",
    "    history,model = train_network(model, 'asc\\\\2D_models\\\\f'+str(fold)+'-best_model-strongFramesV2', X_train, Y_train, X_val, Y_val, nb_epoch, valSplit_size, batch_size, early_stoping_patience,'.')\n",
    "    model.save_weights(savemodelfilename)\n",
    "    return model, history\n",
    "\n",
    "#testruncode - all fold run in cell below\n",
    "#savemodelfilename='2D_models\\\\f'+str(fold)+'_model.testsave'\n",
    "#samples=(X_train.shape[0])\n",
    "#print(X_train.shape)\n",
    "#models[fold],histories[fold] = buildModel(savemodelfilename, samples,model,X_train_mfcc,Y_train_mfcc,X_val_mfcc, Y_val_mfcc,)\n",
    "#models,histories= buildModel(savemodelfilename, samples,model,X_train,Y_train,X_val, Y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[6]\n",
      "[2]\n",
      "training list is 2100 long\n",
      "validation list is 300 long\n",
      "data obtained\n",
      "lists sorted\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 1292, 128, 256)    2560      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 646, 64, 256)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 646, 64, 256)      1024      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 646, 64, 256)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 646, 64, 512)      1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 323, 32, 512)      0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 323, 32, 512)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 323, 32, 1292)     2647308   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 162, 16, 1292)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 162, 16, 1292)     5168      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 162, 16, 1292)     0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 1292, 2592)        0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 1292, 1292)        20077680  \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 1292, 1292)        1670556   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1292, 1292)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1292, 1292)        5168      \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 1292, 10)          12930     \n",
      "=================================================================\n",
      "Total params: 25,602,554\n",
      "Trainable params: 25,596,874\n",
      "Non-trainable params: 5,680\n",
      "_________________________________________________________________\n",
      "model compiled\n",
      "2019-03-24 19:20:40.934994\n",
      "Train on 2100 samples, validate on 300 samples\n",
      "Epoch 1/100\n",
      "2100/2100 [==============================] - 1075s 512ms/step - loss: 0.4131 - binary_accuracy: 0.8382 - val_loss: 0.3454 - val_binary_accuracy: 0.8848\n",
      "Epoch 2/100\n",
      "2100/2100 [==============================] - 1070s 510ms/step - loss: 0.2601 - binary_accuracy: 0.9041 - val_loss: 0.2993 - val_binary_accuracy: 0.9057\n",
      "Epoch 3/100\n",
      "2100/2100 [==============================] - 1070s 510ms/step - loss: 0.1825 - binary_accuracy: 0.9276 - val_loss: 0.2745 - val_binary_accuracy: 0.9144\n",
      "Epoch 4/100\n",
      "2100/2100 [==============================] - 1070s 509ms/step - loss: 0.1042 - binary_accuracy: 0.9616 - val_loss: 0.3210 - val_binary_accuracy: 0.8971\n",
      "Epoch 5/100\n",
      "2100/2100 [==============================] - 1070s 510ms/step - loss: 0.0594 - binary_accuracy: 0.9803 - val_loss: 0.4146 - val_binary_accuracy: 0.8665\n",
      "Epoch 6/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 0.0253 - binary_accuracy: 0.9931 - val_loss: 0.3956 - val_binary_accuracy: 0.8856\n",
      "Epoch 7/100\n",
      "2100/2100 [==============================] - 1067s 508ms/step - loss: 0.0132 - binary_accuracy: 0.9970 - val_loss: 0.6221 - val_binary_accuracy: 0.8697\n",
      "Epoch 8/100\n",
      "2100/2100 [==============================] - 1070s 509ms/step - loss: 0.0148 - binary_accuracy: 0.9959 - val_loss: 0.5074 - val_binary_accuracy: 0.9013\n",
      "Epoch 9/100\n",
      "2100/2100 [==============================] - 1067s 508ms/step - loss: 0.0237 - binary_accuracy: 0.9925 - val_loss: 0.4272 - val_binary_accuracy: 0.8755\n",
      "Epoch 10/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 0.0202 - binary_accuracy: 0.9939 - val_loss: 0.5506 - val_binary_accuracy: 0.8566\n",
      "Epoch 11/100\n",
      "2100/2100 [==============================] - 1071s 510ms/step - loss: 0.0079 - binary_accuracy: 0.9982 - val_loss: 0.5911 - val_binary_accuracy: 0.8724\n",
      "Epoch 12/100\n",
      "2100/2100 [==============================] - 1070s 510ms/step - loss: 0.0049 - binary_accuracy: 0.9989 - val_loss: 0.6607 - val_binary_accuracy: 0.8730\n",
      "Epoch 13/100\n",
      "2100/2100 [==============================] - 1068s 509ms/step - loss: 0.0031 - binary_accuracy: 0.9994 - val_loss: 0.7068 - val_binary_accuracy: 0.8668\n",
      "Epoch 14/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 0.0227 - binary_accuracy: 0.9930 - val_loss: 0.6368 - val_binary_accuracy: 0.8670\n",
      "Epoch 15/100\n",
      "2100/2100 [==============================] - 1070s 509ms/step - loss: 0.0307 - binary_accuracy: 0.9906 - val_loss: 0.5165 - val_binary_accuracy: 0.8830\n",
      "Epoch 16/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 0.0111 - binary_accuracy: 0.9974 - val_loss: 0.6649 - val_binary_accuracy: 0.8616\n",
      "Epoch 17/100\n",
      "2100/2100 [==============================] - 1068s 509ms/step - loss: 0.0032 - binary_accuracy: 0.9995 - val_loss: 0.7510 - val_binary_accuracy: 0.8550\n",
      "Epoch 18/100\n",
      "2100/2100 [==============================] - 1070s 509ms/step - loss: 0.0028 - binary_accuracy: 0.9994 - val_loss: 0.8436 - val_binary_accuracy: 0.8532\n",
      "Epoch 19/100\n",
      "2100/2100 [==============================] - 1070s 509ms/step - loss: 0.0016 - binary_accuracy: 0.9997 - val_loss: 0.8700 - val_binary_accuracy: 0.8564\n",
      "Epoch 20/100\n",
      "2100/2100 [==============================] - 1068s 509ms/step - loss: 0.0021 - binary_accuracy: 0.9995 - val_loss: 0.8880 - val_binary_accuracy: 0.8604\n",
      "Epoch 21/100\n",
      "2100/2100 [==============================] - 1068s 509ms/step - loss: 0.0013 - binary_accuracy: 0.9998 - val_loss: 0.9084 - val_binary_accuracy: 0.8547\n",
      "Epoch 22/100\n",
      "2100/2100 [==============================] - 1070s 509ms/step - loss: 0.0433 - binary_accuracy: 0.9879 - val_loss: 0.4069 - val_binary_accuracy: 0.8604\n",
      "Epoch 23/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 0.0134 - binary_accuracy: 0.9964 - val_loss: 0.6455 - val_binary_accuracy: 0.8539\n",
      "Epoch 24/100\n",
      "2100/2100 [==============================] - 1070s 509ms/step - loss: 0.0148 - binary_accuracy: 0.9958 - val_loss: 0.6480 - val_binary_accuracy: 0.8763\n",
      "Epoch 25/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 0.0166 - binary_accuracy: 0.9954 - val_loss: 0.7939 - val_binary_accuracy: 0.8634\n",
      "Epoch 26/100\n",
      "2100/2100 [==============================] - 1068s 509ms/step - loss: 0.0160 - binary_accuracy: 0.9956 - val_loss: 0.5465 - val_binary_accuracy: 0.8579\n",
      "Epoch 27/100\n",
      "2100/2100 [==============================] - 1068s 509ms/step - loss: 0.0050 - binary_accuracy: 0.9987 - val_loss: 0.8271 - val_binary_accuracy: 0.8662\n",
      "Epoch 28/100\n",
      "2100/2100 [==============================] - 1070s 509ms/step - loss: 0.0187 - binary_accuracy: 0.9949 - val_loss: 0.6562 - val_binary_accuracy: 0.8775\n",
      "Epoch 29/100\n",
      "2100/2100 [==============================] - 1070s 510ms/step - loss: 0.0073 - binary_accuracy: 0.9981 - val_loss: 0.7255 - val_binary_accuracy: 0.8709\n",
      "Epoch 30/100\n",
      "2100/2100 [==============================] - 1067s 508ms/step - loss: 0.0038 - binary_accuracy: 0.9991 - val_loss: 0.8207 - val_binary_accuracy: 0.8692\n",
      "Epoch 31/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 0.0019 - binary_accuracy: 0.9996 - val_loss: 0.8513 - val_binary_accuracy: 0.8748\n",
      "Epoch 32/100\n",
      "2100/2100 [==============================] - 1070s 509ms/step - loss: 0.0164 - binary_accuracy: 0.9956 - val_loss: 0.6820 - val_binary_accuracy: 0.8734\n",
      "Epoch 33/100\n",
      "2100/2100 [==============================] - 1070s 509ms/step - loss: 0.0060 - binary_accuracy: 0.9983 - val_loss: 0.8249 - val_binary_accuracy: 0.8801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100\n",
      "2100/2100 [==============================] - 1068s 509ms/step - loss: 0.0026 - binary_accuracy: 0.9995 - val_loss: 0.9678 - val_binary_accuracy: 0.8600\n",
      "Epoch 35/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 0.0044 - binary_accuracy: 0.9991 - val_loss: 0.9473 - val_binary_accuracy: 0.8724\n",
      "Epoch 36/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 0.0018 - binary_accuracy: 0.9996 - val_loss: 0.9792 - val_binary_accuracy: 0.8617\n",
      "Epoch 37/100\n",
      "2100/2100 [==============================] - 1068s 509ms/step - loss: 0.0014 - binary_accuracy: 0.9997 - val_loss: 0.9842 - val_binary_accuracy: 0.8615\n",
      "Epoch 38/100\n",
      "2100/2100 [==============================] - 1068s 509ms/step - loss: 0.0011 - binary_accuracy: 0.9998 - val_loss: 0.9785 - val_binary_accuracy: 0.8715\n",
      "Epoch 39/100\n",
      "2100/2100 [==============================] - 1070s 510ms/step - loss: 8.9843e-04 - binary_accuracy: 0.9998 - val_loss: 1.0418 - val_binary_accuracy: 0.8658\n",
      "Epoch 40/100\n",
      "2100/2100 [==============================] - 1070s 510ms/step - loss: 6.4043e-04 - binary_accuracy: 0.9999 - val_loss: 1.1440 - val_binary_accuracy: 0.8579\n",
      "Epoch 41/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 0.0358 - binary_accuracy: 0.9901 - val_loss: 0.5673 - val_binary_accuracy: 0.8468\n",
      "Epoch 42/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 0.0126 - binary_accuracy: 0.9967 - val_loss: 0.6213 - val_binary_accuracy: 0.8787\n",
      "Epoch 43/100\n",
      "2100/2100 [==============================] - 1068s 509ms/step - loss: 0.0105 - binary_accuracy: 0.9974 - val_loss: 0.8673 - val_binary_accuracy: 0.8662\n",
      "Epoch 44/100\n",
      "2100/2100 [==============================] - 1070s 509ms/step - loss: 0.0039 - binary_accuracy: 0.9991 - val_loss: 0.9287 - val_binary_accuracy: 0.8728\n",
      "Epoch 45/100\n",
      "2100/2100 [==============================] - 1071s 510ms/step - loss: 0.0024 - binary_accuracy: 0.9995 - val_loss: 0.9502 - val_binary_accuracy: 0.8785\n",
      "Epoch 46/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 0.0018 - binary_accuracy: 0.9997 - val_loss: 0.9793 - val_binary_accuracy: 0.8813\n",
      "Epoch 47/100\n",
      "2100/2100 [==============================] - 1070s 510ms/step - loss: 0.0014 - binary_accuracy: 0.9998 - val_loss: 1.0204 - val_binary_accuracy: 0.8794\n",
      "Epoch 48/100\n",
      "2100/2100 [==============================] - 1070s 509ms/step - loss: 0.0015 - binary_accuracy: 0.9997 - val_loss: 1.0516 - val_binary_accuracy: 0.8720\n",
      "Epoch 49/100\n",
      "2100/2100 [==============================] - 1068s 509ms/step - loss: 0.0011 - binary_accuracy: 0.9998 - val_loss: 1.0750 - val_binary_accuracy: 0.8780\n",
      "Epoch 50/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 0.0019 - binary_accuracy: 0.9996 - val_loss: 1.1823 - val_binary_accuracy: 0.8780\n",
      "Epoch 51/100\n",
      "2100/2100 [==============================] - 1070s 509ms/step - loss: 9.0093e-04 - binary_accuracy: 0.9998 - val_loss: 1.2215 - val_binary_accuracy: 0.8729\n",
      "Epoch 52/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 0.0024 - binary_accuracy: 0.9993 - val_loss: 1.4333 - val_binary_accuracy: 0.8625\n",
      "Epoch 53/100\n",
      "2100/2100 [==============================] - 1068s 508ms/step - loss: 7.6876e-04 - binary_accuracy: 0.9999 - val_loss: 1.4019 - val_binary_accuracy: 0.8655\n",
      "Epoch 54/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 5.2457e-04 - binary_accuracy: 0.9999 - val_loss: 1.4162 - val_binary_accuracy: 0.8633\n",
      "Epoch 55/100\n",
      "2100/2100 [==============================] - 1070s 510ms/step - loss: 5.2000e-04 - binary_accuracy: 0.9999 - val_loss: 1.4179 - val_binary_accuracy: 0.8622\n",
      "Epoch 56/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 0.0011 - binary_accuracy: 0.9997 - val_loss: 1.4506 - val_binary_accuracy: 0.8634\n",
      "Epoch 57/100\n",
      "2100/2100 [==============================] - 1070s 509ms/step - loss: 0.0591 - binary_accuracy: 0.9845 - val_loss: 0.8184 - val_binary_accuracy: 0.7333\n",
      "Epoch 58/100\n",
      "2100/2100 [==============================] - 1068s 508ms/step - loss: 0.0261 - binary_accuracy: 0.9926 - val_loss: 0.7079 - val_binary_accuracy: 0.8611\n",
      "Epoch 59/100\n",
      "2100/2100 [==============================] - 1068s 509ms/step - loss: 0.0090 - binary_accuracy: 0.9980 - val_loss: 0.7652 - val_binary_accuracy: 0.8618\n",
      "Epoch 60/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 0.0039 - binary_accuracy: 0.9993 - val_loss: 0.8556 - val_binary_accuracy: 0.8577\n",
      "Epoch 61/100\n",
      "2100/2100 [==============================] - 1068s 509ms/step - loss: 0.0033 - binary_accuracy: 0.9993 - val_loss: 0.7748 - val_binary_accuracy: 0.8694\n",
      "Epoch 62/100\n",
      "2100/2100 [==============================] - 1071s 510ms/step - loss: 0.0047 - binary_accuracy: 0.9990 - val_loss: 0.8271 - val_binary_accuracy: 0.8770\n",
      "Epoch 63/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 0.0019 - binary_accuracy: 0.9997 - val_loss: 0.9060 - val_binary_accuracy: 0.8620\n",
      "Epoch 64/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 0.0015 - binary_accuracy: 0.9997 - val_loss: 1.0697 - val_binary_accuracy: 0.8546\n",
      "Epoch 65/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 0.0010 - binary_accuracy: 0.9999 - val_loss: 1.0969 - val_binary_accuracy: 0.8603\n",
      "Epoch 66/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 0.0022 - binary_accuracy: 0.9994 - val_loss: 1.1006 - val_binary_accuracy: 0.8693\n",
      "Epoch 67/100\n",
      "2100/2100 [==============================] - 1070s 510ms/step - loss: 0.0016 - binary_accuracy: 0.9996 - val_loss: 1.1785 - val_binary_accuracy: 0.8639\n",
      "Epoch 68/100\n",
      "2100/2100 [==============================] - 1068s 509ms/step - loss: 8.6315e-04 - binary_accuracy: 0.9998 - val_loss: 1.2271 - val_binary_accuracy: 0.8641\n",
      "Epoch 69/100\n",
      "2100/2100 [==============================] - 1070s 509ms/step - loss: 0.0032 - binary_accuracy: 0.9994 - val_loss: 1.0958 - val_binary_accuracy: 0.8589\n",
      "Epoch 70/100\n",
      "2100/2100 [==============================] - 1068s 509ms/step - loss: 0.0307 - binary_accuracy: 0.9913 - val_loss: 0.7574 - val_binary_accuracy: 0.8708\n",
      "Epoch 71/100\n",
      "2100/2100 [==============================] - 1068s 509ms/step - loss: 0.0054 - binary_accuracy: 0.9989 - val_loss: 0.8754 - val_binary_accuracy: 0.8690\n",
      "Epoch 72/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 0.0025 - binary_accuracy: 0.9995 - val_loss: 0.9058 - val_binary_accuracy: 0.8678\n",
      "Epoch 73/100\n",
      "2100/2100 [==============================] - 1068s 509ms/step - loss: 0.0024 - binary_accuracy: 0.9994 - val_loss: 0.9945 - val_binary_accuracy: 0.8539\n",
      "Epoch 74/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 0.0025 - binary_accuracy: 0.9993 - val_loss: 1.0122 - val_binary_accuracy: 0.8569\n",
      "Epoch 75/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 0.0011 - binary_accuracy: 0.9998 - val_loss: 1.0705 - val_binary_accuracy: 0.8587\n",
      "Epoch 76/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 0.0010 - binary_accuracy: 0.9999 - val_loss: 1.0994 - val_binary_accuracy: 0.8602\n",
      "Epoch 77/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 8.4936e-04 - binary_accuracy: 0.9999 - val_loss: 1.0840 - val_binary_accuracy: 0.8611\n",
      "Epoch 78/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 0.0013 - binary_accuracy: 0.9998 - val_loss: 1.1649 - val_binary_accuracy: 0.8560\n",
      "Epoch 79/100\n",
      "2100/2100 [==============================] - 1068s 508ms/step - loss: 0.0019 - binary_accuracy: 0.9995 - val_loss: 1.1929 - val_binary_accuracy: 0.8594\n",
      "Epoch 80/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 8.7833e-04 - binary_accuracy: 0.9999 - val_loss: 1.2254 - val_binary_accuracy: 0.8627\n",
      "Epoch 81/100\n",
      "2100/2100 [==============================] - 1070s 510ms/step - loss: 0.0011 - binary_accuracy: 0.9997 - val_loss: 1.1774 - val_binary_accuracy: 0.8586\n",
      "Epoch 82/100\n",
      "2100/2100 [==============================] - 1071s 510ms/step - loss: 8.7812e-04 - binary_accuracy: 0.9998 - val_loss: 1.1136 - val_binary_accuracy: 0.8734\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 5.1318e-04 - binary_accuracy: 0.9999 - val_loss: 1.2461 - val_binary_accuracy: 0.8701\n",
      "Epoch 84/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 3.6422e-04 - binary_accuracy: 1.0000 - val_loss: 1.3068 - val_binary_accuracy: 0.8698\n",
      "Epoch 85/100\n",
      "2100/2100 [==============================] - 1070s 509ms/step - loss: 5.1269e-04 - binary_accuracy: 0.9999 - val_loss: 1.3219 - val_binary_accuracy: 0.8703\n",
      "Epoch 86/100\n",
      "2100/2100 [==============================] - 1068s 508ms/step - loss: 4.5352e-04 - binary_accuracy: 0.9999 - val_loss: 1.3737 - val_binary_accuracy: 0.8665\n",
      "Epoch 87/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 4.1946e-04 - binary_accuracy: 0.9999 - val_loss: 1.3031 - val_binary_accuracy: 0.8687\n",
      "Epoch 88/100\n",
      "2100/2100 [==============================] - 1070s 510ms/step - loss: 4.8351e-04 - binary_accuracy: 0.9999 - val_loss: 1.4881 - val_binary_accuracy: 0.8640\n",
      "Epoch 89/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 3.8322e-04 - binary_accuracy: 0.9999 - val_loss: 1.5806 - val_binary_accuracy: 0.8606\n",
      "Epoch 90/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 4.9148e-04 - binary_accuracy: 0.9999 - val_loss: 1.5528 - val_binary_accuracy: 0.8602\n",
      "Epoch 91/100\n",
      "2100/2100 [==============================] - 1068s 508ms/step - loss: 3.1465e-04 - binary_accuracy: 1.0000 - val_loss: 1.6370 - val_binary_accuracy: 0.8582\n",
      "Epoch 92/100\n",
      "2100/2100 [==============================] - 1068s 509ms/step - loss: 2.6660e-04 - binary_accuracy: 1.0000 - val_loss: 1.4400 - val_binary_accuracy: 0.8681\n",
      "Epoch 93/100\n",
      "2100/2100 [==============================] - 1071s 510ms/step - loss: 6.0089e-04 - binary_accuracy: 0.9999 - val_loss: 1.5669 - val_binary_accuracy: 0.8715\n",
      "Epoch 94/100\n",
      "2100/2100 [==============================] - 1068s 509ms/step - loss: 3.0308e-04 - binary_accuracy: 1.0000 - val_loss: 1.6221 - val_binary_accuracy: 0.8698\n",
      "Epoch 95/100\n",
      "2100/2100 [==============================] - 1070s 509ms/step - loss: 4.9915e-04 - binary_accuracy: 0.9999 - val_loss: 1.7781 - val_binary_accuracy: 0.8464\n",
      "Epoch 96/100\n",
      "2100/2100 [==============================] - 1068s 509ms/step - loss: 0.0231 - binary_accuracy: 0.9967 - val_loss: 1.8655 - val_binary_accuracy: 0.8248\n",
      "Epoch 97/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 0.0413 - binary_accuracy: 0.9888 - val_loss: 0.9207 - val_binary_accuracy: 0.8411\n",
      "Epoch 98/100\n",
      "2100/2100 [==============================] - 1067s 508ms/step - loss: 0.0070 - binary_accuracy: 0.9982 - val_loss: 0.9781 - val_binary_accuracy: 0.8426\n",
      "Epoch 99/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 0.0093 - binary_accuracy: 0.9975 - val_loss: 1.2678 - val_binary_accuracy: 0.8430\n",
      "Epoch 100/100\n",
      "2100/2100 [==============================] - 1069s 509ms/step - loss: 0.0028 - binary_accuracy: 0.9993 - val_loss: 1.2052 - val_binary_accuracy: 0.8377\n",
      "2019-03-26 01:02:59.880246\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "batch_size =  8\n",
    "nb_classes = 10\n",
    "rows, cols = 1292, 128\n",
    "nb_epoch = 100\n",
    "pool_size = (3,3)                  # size of pooling area for max pooling\n",
    "prob_drop_conv = 0.25                # drop probability for dropout @ conv layer\n",
    "prob_drop_hidden = 0.5              # drop probability for dropout @ fc layer\n",
    "context='asc'\n",
    "\n",
    "for fold in range(1,2):\n",
    "    print(fold)\n",
    "    #sort data\n",
    "    X_train, Y_train, tf_list, vf_list = getTrainDataForFold(fold,context)\n",
    "    X_train=np.expand_dims(X_train,axis=3)\n",
    "    print('data obtained')\n",
    "    X_val,Y_val=getValDataForFold(vf_list,context)\n",
    "    X_val=np.expand_dims(X_val,axis=3)\n",
    "   # src_files='stacked_for_classification\\\\test\\\\'+str(fold)+'\\\\*2channel.pckl'\n",
    "   # tef_list=glob.glob(src_files)\n",
    "   # X_test,Y_test=getTestDataForFold(tef_list,context)\n",
    "  #  X_test=np.expand_dims(X_test,axis=3) \n",
    "    print('lists sorted')\n",
    "    model = compileCRNN(cols,rows,channels,nb_classes=nb_classes)\n",
    "    print('model compiled')\n",
    "    savemodelfilename=context+'\\\\2D_models\\\\f'+str(fold)+'_model-strongFramesV2.testsave'\n",
    "    samples=(X_train.shape[0])\n",
    "    print(datetime.now())\n",
    "    models,histories= buildModel(savemodelfilename, samples,model,X_train,Y_train,X_val, Y_val,fold)\n",
    "    print(datetime.now())\n",
    "    print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate - is this sufficient for test? do i need predictions first? check\n",
    "#evaluation = model.evaluate(X_test_lmsSwapped_4d, Y_test_lms, batch_size=s_batch_size, verbose=1)\n",
    "#print('Summary: Loss over the test dataset: %.4f, Accuracy: %.4f' % (evaluation[0], evaluation[1]))\n",
    "\n",
    "# reload best model and rerun evaluation\n",
    "#print('loading best model...')\n",
    "#best_model = load_model('2016ASC_f'+str(fold)+'-lms_model.hdf5')\n",
    "\n",
    "#evaluation = best_model.evaluate(X_test_lmsSwapped_4d, Y_test_lms, batch_size=s_batch_size, verbose=1)\n",
    "#print('Summary: Loss over the test dataset: %.4f, Accuracy: %.4f' % (evaluation[0], evaluation[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proper ASC - majority voting prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proper SED - on/off set detection? \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
